{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Desmond-Tiny/cnn-cat-dog-detector/blob/main/CAT%26DOGS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huqVR6YoDagy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cats & Dogs Dataset"
      ],
      "metadata": {
        "id": "WK9ZBYnql_sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "mNLKrqh4fygq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget google.com\n"
      ],
      "metadata": {
        "id": "cv6ojG14gGxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb88dddc-3033-4924-c79e-75644a749fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-09 11:48:52--  http://google.com/\n",
            "Resolving google.com (google.com)... 142.250.101.138, 142.250.101.113, 142.250.101.102, ...\n",
            "Connecting to google.com (google.com)|142.250.101.138|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://www.google.com/ [following]\n",
            "--2025-06-09 11:48:52--  http://www.google.com/\n",
            "Resolving www.google.com (www.google.com)... 142.250.101.103, 142.250.101.99, 142.250.101.105, ...\n",
            "Connecting to www.google.com (www.google.com)|142.250.101.103|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘index.html’\n",
            "\n",
            "\rindex.html              [<=>                 ]       0  --.-KB/s               \rindex.html              [ <=>                ]  16.79K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-09 11:48:52 (129 MB/s) - ‘index.html’ saved [17196]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" \\\n",
        "    -O \"/content/cats-and-dogs.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA1xaF9efxBC",
        "outputId": "b6af893e-ff78-4a76-e107-b322b7c6b5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-09 11:31:43--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 184.29.22.44, 2600:1406:bc00:e8a::317f, 2600:1406:bc00:e85::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|184.29.22.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824887076 (787M) [application/octet-stream]\n",
            "Saving to: ‘/content/cats-and-dogs.zip’\n",
            "\n",
            "/content/cats-and-d 100%[===================>] 786.67M   102MB/s    in 8.8s    \n",
            "\n",
            "2025-06-09 11:31:52 (89.6 MB/s) - ‘/content/cats-and-dogs.zip’ saved [824887076/824887076]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -xq '/content/cats-and-dogs.zip'"
      ],
      "metadata": {
        "id": "xrDZTTieggI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6uRuKT90dnQy",
        "outputId": "d0fe3eab-4630-4f82-e7cc-8686da6a1557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob as g"
      ],
      "metadata": {
        "id": "xJBIJHlwggGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#g('/content/PetImages/*/*')"
      ],
      "metadata": {
        "id": "j5r68f-nfvAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for path in g('/content/PetImages/*/*'):\n",
        "  if not os.path.getsize(path):\n",
        "    count+=1\n",
        "    os.remove(path)\n",
        "\n",
        "count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgQ6cv4ufu9e",
        "outputId": "92160049-1068-4d55-8ab3-d0f56050078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.listdir('/content/PetImages/Cat')"
      ],
      "metadata": {
        "id": "4hNRu_M5fu6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder='/content/dataset'\n"
      ],
      "metadata": {
        "id": "t_WwlkngeqTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cats_dir=f'{folder}/training/cats'\n",
        "train_dogs_dir=f'{folder}/training/dogs'\n",
        "\n",
        "\n",
        "test_cats_dir=f'{folder}/testing/cats'\n",
        "test_dogs_dir=f'{folder}/testing/dogs'\n",
        "\n",
        "folders=[\n",
        "    train_cats_dir,\n",
        "    train_dogs_dir,\n",
        "    test_cats_dir,\n",
        "    test_dogs_dir,\n",
        "]"
      ],
      "metadata": {
        "id": "vVvvMAI4eD04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.mkdir"
      ],
      "metadata": {
        "id": "rFdbskY6iMWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in folders:\n",
        "  os.makedirs(f) # research\n"
      ],
      "metadata": {
        "id": "jq3e7ocEKme8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cats=sorted(g('/content/PetImages/Cat/*'))\n",
        "all_dogs=sorted(g('/content/PetImages/Dog/*'))\n",
        "len(all_cats),len(all_dogs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMN_dZmNiUl5",
        "outputId": "ea2c512f-6cfa-4b0e-f5cb-9f9e8c1861a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dogs_train,dogs_test=train_test_split(all_dogs,test_size=0.1,shuffle=True,random_state=1403)\n",
        "len(dogs_train),len(dogs_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEA3XPl9iUhY",
        "outputId": "54f1184f-78ee-4092-b690-a421144c2d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11250, 1250)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cats_train,cats_test=train_test_split(all_cats,test_size=0.1,shuffle=True,random_state=1403)\n",
        "len(cats_train),len(cats_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbmpafI_iUex",
        "outputId": "28414f6f-240e-4588-b44a-9ef496e85ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11250, 1250)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dogs_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tA66bXAwiUcK",
        "outputId": "24b3c31c-13e3-41e7-c1f2-c9fa9403cfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/PetImages/Dog/9388.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "LdWr63tzjpXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in dogs_train:\n",
        "  shutil.copy(name,os.path.join(train_dogs_dir,name.split('/')[-1]))"
      ],
      "metadata": {
        "id": "mtQWfR41iySr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(train_dogs_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcQgEiKIiyQC",
        "outputId": "92269e4b-6489-4d39-800c-55e06b020cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11250"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in dogs_test:\n",
        "  shutil.copy(name,os.path.join(test_dogs_dir,name.split('/')[-1]))\n",
        "\n",
        "\n",
        "for name in cats_train:\n",
        "  shutil.copy(name,os.path.join(train_cats_dir,name.split('/')[-1]))\n",
        "\n",
        "for name in cats_test:\n",
        "  shutil.copy(name,os.path.join(test_cats_dir,name.split('/')[-1]))"
      ],
      "metadata": {
        "id": "iAjJpLgLiyNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(test_dogs_dir)),len(os.listdir(train_cats_dir)),len(os.listdir(test_cats_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsUqTALNiUZY",
        "outputId": "b3db885c-84cb-4345-945e-8513d4b7eb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1250, 11250, 1250)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dir='/content/dataset/training'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(training_dir,batch_size=64,class_mode='binary',target_size=(150,150))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR_h0G_eiUWn",
        "outputId": "7cf82a07-7053-4381-9728-5e0e8f90eaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22498 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dir='/content/dataset/testing'\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(testing_dir,batch_size=64,class_mode='binary',target_size=(150,150))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aYDkp0ziUTo",
        "outputId": "5f9a93bb-ca01-4394-c471-c618db7526f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# CNN\n",
        "\n",
        "\n",
        "# sigmoid\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Fu_fxKxpiUQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model7 = tf.keras.Sequential()\n",
        "\n",
        "model7.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
        "model7.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model7.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model7.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model7.add(Flatten())\n",
        "model7.add(Dense(64, activation='relu'))\n",
        "model7.add(Dense(32, activation='relu'))\n",
        "model7.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model7.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "A_w9gG1-oLMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc930400-9994-4199-8b64-cdafee1fd48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model7.fit(train_generator,epochs=15,verbose=1)"
      ],
      "metadata": {
        "id": "fWap2i1eKmcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d874984-0010-4f80-cbfc-5cd995dffbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m104/352\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 123ms/step - accuracy: 0.5451 - loss: 0.7517"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 150ms/step - accuracy: 0.6157 - loss: 0.6609\n",
            "Epoch 2/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 139ms/step - accuracy: 0.7821 - loss: 0.4582\n",
            "Epoch 3/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.8624 - loss: 0.3209\n",
            "Epoch 4/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 122ms/step - accuracy: 0.9490 - loss: 0.1414\n",
            "Epoch 5/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.9866 - loss: 0.0434\n",
            "Epoch 6/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - accuracy: 0.9893 - loss: 0.0338\n",
            "Epoch 7/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9930 - loss: 0.0230\n",
            "Epoch 8/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - accuracy: 0.9952 - loss: 0.0156\n",
            "Epoch 9/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 119ms/step - accuracy: 0.9959 - loss: 0.0128\n",
            "Epoch 10/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9969 - loss: 0.0114\n",
            "Epoch 11/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 118ms/step - accuracy: 0.9943 - loss: 0.0172\n",
            "Epoch 12/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9982 - loss: 0.0065\n",
            "Epoch 13/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 118ms/step - accuracy: 0.9972 - loss: 0.0103\n",
            "Epoch 14/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9955 - loss: 0.0147\n",
            "Epoch 15/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.9964 - loss: 0.0099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_proba = model7.predict(test_generator)\n",
        "preds = (y_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Get true labels from test generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, preds, target_names=test_generator.class_indices.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS7RPW_5s09O",
        "outputId": "8926f46d-e0b9-45ee-a2a1-91ee1a0c809b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.50      0.59      0.54      1250\n",
            "        dogs       0.51      0.43      0.46      1250\n",
            "\n",
            "    accuracy                           0.51      2500\n",
            "   macro avg       0.51      0.51      0.50      2500\n",
            "weighted avg       0.51      0.51      0.50      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model8 = tf.keras.Sequential()\n",
        "\n",
        "model8.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
        "model8.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model8.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "model8.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model8.add(Dropout(0.25))  # Dropout after max-pooling\n",
        "\n",
        "model8.add(Flatten())\n",
        "model8.add(Dense(64, activation='relu'))\n",
        "model8.add(Dropout(0.5))  # Dropout after first dense layer\n",
        "model8.add(Dense(32, activation='relu'))\n",
        "model8.add(Dropout(0.5))  # Dropout after second dense layer\n",
        "model8.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model8.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "2YXELLaBh8Um",
        "outputId": "4515c58a-d7fc-42ce-f372-26d1b0ea194a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model8.fit(train_generator,epochs=15,verbose=1)"
      ],
      "metadata": {
        "id": "Yb6M1iA6iMF3",
        "outputId": "ee8b8571-5506-4c3b-af1c-c5f9d795a9d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m263/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 146ms/step - accuracy: 0.4993 - loss: 0.8433"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 140ms/step - accuracy: 0.4982 - loss: 0.8117\n",
            "Epoch 2/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - accuracy: 0.5124 - loss: 0.6922\n",
            "Epoch 3/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - accuracy: 0.5620 - loss: 0.6832\n",
            "Epoch 4/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - accuracy: 0.6456 - loss: 0.6442\n",
            "Epoch 5/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 122ms/step - accuracy: 0.7009 - loss: 0.5877\n",
            "Epoch 6/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 123ms/step - accuracy: 0.7777 - loss: 0.4897\n",
            "Epoch 7/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 120ms/step - accuracy: 0.8279 - loss: 0.4089\n",
            "Epoch 8/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.8591 - loss: 0.3429\n",
            "Epoch 9/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.8880 - loss: 0.2842\n",
            "Epoch 10/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - accuracy: 0.9066 - loss: 0.2464\n",
            "Epoch 11/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.9255 - loss: 0.2135\n",
            "Epoch 12/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 121ms/step - accuracy: 0.9286 - loss: 0.2014\n",
            "Epoch 13/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 123ms/step - accuracy: 0.9340 - loss: 0.1812\n",
            "Epoch 14/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9448 - loss: 0.1610\n",
            "Epoch 15/15\n",
            "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 119ms/step - accuracy: 0.9490 - loss: 0.1515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_proba = model8.predict(test_generator)\n",
        "preds = (y_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "# Get true labels from test generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, preds, target_names=test_generator.class_indices.keys()))"
      ],
      "metadata": {
        "id": "gEOhIl5WiOTq",
        "outputId": "391524eb-d563-41d1-85f7-15ae44d19220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.52      0.49      0.50      1250\n",
            "        dogs       0.52      0.56      0.54      1250\n",
            "\n",
            "    accuracy                           0.52      2500\n",
            "   macro avg       0.52      0.52      0.52      2500\n",
            "weighted avg       0.52      0.52      0.52      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Image size\n",
        "IMG_SIZE = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # use part of training data for validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Test set (no augmentation, just rescaling)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    testing_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load pre-trained MobileNetV2 without top classifier\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # freeze base model\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Define full model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "6-neNzJ-lmYS",
        "outputId": "feab0e91-5cfd-4e9b-c2c2-4308f90c10f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18000 images belonging to 2 classes.\n",
            "Found 4498 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-45589f82ef23>:52: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m370/563\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 232ms/step - accuracy: 0.7439 - loss: 0.5640"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 297ms/step - accuracy: 0.7775 - loss: 0.4933 - val_accuracy: 0.9133 - val_loss: 0.2042\n",
            "Epoch 2/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 274ms/step - accuracy: 0.8989 - loss: 0.2357 - val_accuracy: 0.9202 - val_loss: 0.1798\n",
            "Epoch 3/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 268ms/step - accuracy: 0.9059 - loss: 0.2136 - val_accuracy: 0.9269 - val_loss: 0.1762\n",
            "Epoch 4/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 267ms/step - accuracy: 0.9128 - loss: 0.1998 - val_accuracy: 0.9300 - val_loss: 0.1723\n",
            "Epoch 5/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 286ms/step - accuracy: 0.9144 - loss: 0.1982 - val_accuracy: 0.9315 - val_loss: 0.1664\n",
            "Epoch 6/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 267ms/step - accuracy: 0.9139 - loss: 0.1906 - val_accuracy: 0.9273 - val_loss: 0.1689\n",
            "Epoch 7/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 266ms/step - accuracy: 0.9182 - loss: 0.1852 - val_accuracy: 0.9344 - val_loss: 0.1625\n",
            "Epoch 8/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 264ms/step - accuracy: 0.9198 - loss: 0.1893 - val_accuracy: 0.9291 - val_loss: 0.1666\n",
            "Epoch 9/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 262ms/step - accuracy: 0.9215 - loss: 0.1851 - val_accuracy: 0.9295 - val_loss: 0.1698\n",
            "Epoch 10/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 264ms/step - accuracy: 0.9269 - loss: 0.1712 - val_accuracy: 0.9306 - val_loss: 0.1643\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.9498 - loss: 0.1225\n",
            "Test accuracy: 0.9592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "Y_pred = model.predict(test_generator)\n",
        "y_pred = np.round(Y_pred).astype(int).reshape(-1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=['cats', 'dogs']))\n"
      ],
      "metadata": {
        "id": "1Au6_l7ItmFC",
        "outputId": "8b358d57-aa03-4628-96ad-d87950bf5001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 136ms/step\n",
            "[[1176   74]\n",
            " [  28 1222]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.98      0.94      0.96      1250\n",
            "        dogs       0.94      0.98      0.96      1250\n",
            "\n",
            "    accuracy                           0.96      2500\n",
            "   macro avg       0.96      0.96      0.96      2500\n",
            "weighted avg       0.96      0.96      0.96      2500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}